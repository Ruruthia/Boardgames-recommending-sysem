{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "absolute-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from lightfm.evaluation import precision_at_k, recall_at_k, reciprocal_rank, auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-universal",
   "metadata": {},
   "source": [
    "### Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "acquired-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../LightFM_ratings.csv.gz',  usecols=[1,2])\n",
    "# Nazwy najpopularniejszych kategorii\n",
    "features_names = pd.read_csv('../LightFM_item_features_names.csv.gz')\n",
    "# Kategorie konkretnych gier \n",
    "game_features = pd.read_csv('../LightFM_item_features.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "greenhouse-garage",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(by='bgg_user_name')\n",
    "cleaned_df = []\n",
    "for user, user_df in grouped:\n",
    "    if user_df.shape[0] < 10:\n",
    "        continue\n",
    "    cleaned_df.append(user_df[:])\n",
    "cleaned_df = pd.concat(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "rough-mapping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bgg_user_name\n",
       " mycroft           62\n",
       "- v -              12\n",
       "-=yod@=-          307\n",
       "-grizzly-          29\n",
       "-johnny-          125\n",
       "                 ... \n",
       "zzzxxxyyy          20\n",
       "zzzzz              37\n",
       "zzzzzane          169\n",
       "zzzzzyy            11\n",
       "æleksandr þræð     11\n",
       "Name: bgg_id, Length: 247376, dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.groupby('bgg_user_name')['bgg_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "existing-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = [x for x in features_names['0']]\n",
    "games_list = [x for x in cleaned_df['bgg_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "adverse-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the features of games for which we don't have any interaction\n",
    "game_features = game_features.drop((game_features['bgg_id'])[~game_features['bgg_id'].isin(games_list)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-stephen",
   "metadata": {},
   "source": [
    "### Preparation of interactions and item features matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "spatial-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.fit((x for x in cleaned_df['bgg_user_name']), (x for x in cleaned_df['bgg_id']), item_features=(x for x in features_names['0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "institutional-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Won't work with weird designers' names! (like \"..., Jr.\")\n",
    "item_features = dataset.build_item_features((val['bgg_id'], [w[1:-1].replace(\"\\\\'\", \"'\") for w in val['features'][1:-1].split(\", \") if w[1:-1].replace(\"\\\\'\", \"'\") in features_list]) for idx, val in game_features.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "informed-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_interactions(df, dataset):\n",
    "    \n",
    "    def split_test(test_df, seed=42, frac=0.8):\n",
    "        grouped = test_df.groupby(by='bgg_user_name')\n",
    "        test_known = []\n",
    "        test_unknown = []\n",
    "        for user, df in grouped:\n",
    "            df_size = df.shape[0]\n",
    "            known_size = int(round(frac*df_size))\n",
    "            known_indices = np.random.choice(df_size, known_size, replace=False)\n",
    "            known_data = df.iloc[known_indices]\n",
    "            test_known.append(known_data)\n",
    "\n",
    "            unknown_indices = np.setdiff1d(np.arange(df_size), known_indices)\n",
    "            unknown_data = df.iloc[unknown_indices]\n",
    "            test_unknown.append(unknown_data)\n",
    "        return pd.concat(test_known), pd.concat(test_unknown)\n",
    "\n",
    "    users = df['bgg_user_name'].unique()\n",
    "    np.random.shuffle(users)\n",
    "    train_size = int(0.7*users.shape[0])\n",
    "    train_df = df[df['bgg_user_name'].isin(users[:train_size])]\n",
    "    test_df = df[df['bgg_user_name'].isin(users[train_size:])]\n",
    "    print('Splitting test set')\n",
    "    test_known, test_unknown = split_test(test_df)\n",
    "    interactions_df = train_df.append(test_known)\n",
    "    print('Preparing training interactions')\n",
    "    interactions = dataset.build_interactions(((val['bgg_user_name'], val['bgg_id']) for idx, val in interactions_df.iterrows()))\n",
    "    print('Preparing testing interactions')\n",
    "    test_interactions = dataset.build_interactions(((val['bgg_user_name'], val['bgg_id']) for idx, val in test_unknown.iterrows()))\n",
    "    return interactions[0], test_interactions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-blade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting test set\n"
     ]
    }
   ],
   "source": [
    "interactions, test_interactions = prepare_interactions(cleaned_df, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-short",
   "metadata": {},
   "source": [
    "### WARP loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightFM(loss='warp')\n",
    "model.fit(interactions, verbose=True, epochs=20, num_threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_precision = precision_at_k(model, interactions, k=5, num_threads=8).mean()\n",
    "print('Precision: train %.2f' % (train_precision))\n",
    "\n",
    "test_precision = precision_at_k(model, test_interactions, train_interactions = interactions, k=5, num_threads=8).mean()\n",
    "print('Precision: test %.2f' % (test_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc = auc_score(model, interactions, num_threads=8).mean()\n",
    "print('AUC: train %.2f' % (train_auc))\n",
    "\n",
    "test_auc = auc_score(model, test_interactions, train_interactions = interactions, num_threads=8).mean()\n",
    "print('AUC: test %.2f' % (test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-translator",
   "metadata": {},
   "source": [
    "### BPR loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr_model = LightFM(loss='bpr')\n",
    "bpr_model.fit(interactions, verbose=True, epochs=20, num_threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_precision = precision_at_k(bpr_model, interactions, k=5, num_threads=8).mean()\n",
    "print('Precision: train %.2f' % (train_precision))\n",
    "\n",
    "test_precision = precision_at_k(bpr_model, test_interactions, train_interactions = interactions, k=5, num_threads=8).mean()\n",
    "print('Precision: test %.2f' % (test_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc = auc_score(bpr_model, interactions, num_threads=8).mean()\n",
    "print('AUC: train %.2f' % (train_auc))\n",
    "\n",
    "test_auc = auc_score(bpr_model, test_interactions, train_interactions = interactions, num_threads=8).mean()\n",
    "print('AUC: test %.2f' % (test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-briefs",
   "metadata": {},
   "source": [
    "### WARP loss with item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_model = LightFM(loss='warp', no_components = 64, item_alpha = 1e-6)\n",
    "features_model.fit(interactions, verbose=True, item_features = item_features, epochs=20, num_threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_precision = precision_at_k(features_model, interactions, item_features = item_features, k=5, num_threads=8).mean()\n",
    "print('Precision: train %.2f' % (train_precision))\n",
    "\n",
    "test_precision = precision_at_k(features_model, test_interactions, train_interactions = interactions, item_features = item_features, k=5, num_threads=8).mean()\n",
    "print('Precision: test %.2f' % (test_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc = auc_score(features_model, interactions, item_features = item_features, num_threads=8).mean()\n",
    "print('AUC: train %.2f' % (train_auc))\n",
    "\n",
    "test_auc = auc_score(features_model, test_interactions, train_interactions = interactions, item_features = item_features, num_threads=8).mean()\n",
    "print('AUC: test %.2f' % (test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-observation",
   "metadata": {},
   "source": [
    "### Additional evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_recall = recall_at_k(model, interactions, k=5, num_threads=8).mean()\n",
    "print('Recall: train %.2f' % (train_recall))\n",
    "\n",
    "test_recall = recall_at_k(model, test_interactions, train_interactions = interactions, k=5, num_threads=8).mean()\n",
    "print('Recall: test %.2f' % (test_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-sugar",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc = auc_score(model, interactions, num_threads=8).mean()\n",
    "print('AUC: train %.2f' % (train_auc))\n",
    "\n",
    "test_auc = auc_score(model, test_interactions, train_interactions = interactions, num_threads=8).mean()\n",
    "print('AUC: test %.2f' % (test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_recip = reciprocal_rank(model, interactions, num_threads=8).mean()\n",
    "print('Reciprocal rank: train %.2f' % (train_recip))\n",
    "\n",
    "test_recip = reciprocal_rank(model, test_interactions, train_interactions = interactions, num_threads=8).mean()\n",
    "print('Reciprocal rank: test %.2f' % (test_recip))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
